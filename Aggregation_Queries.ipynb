{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ipython-sql psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b64ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd351390",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://postgres:navya@localhost:5432/mydb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4f161",
   "metadata": {},
   "source": [
    "### Insertion into hstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0367a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost:5432/mydb\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS data_hstore CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397906b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 rows in 4840.22 ms\n",
      "Inserted 50000 rows in 25669.60 ms\n",
      "Inserted 100000 rows in 47852.47 ms\n",
      "Inserted 150346 rows in 76198.58 ms\n",
      "\n",
      "Latency Results (HSTORE):\n",
      " 10000 rows →  4840.22 ms\n",
      " 50000 rows → 25669.60 ms\n",
      "100000 rows → 47852.47 ms\n",
      "150346 rows → 76198.58 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "from psycopg2.extras import execute_values, register_hstore\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\",\n",
    "    user=\"postgres\",\n",
    "    password=\"navya\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS hstore;\")\n",
    "register_hstore(conn)\n",
    "\n",
    "cur.execute(\"DROP TABLE IF EXISTS data_hstore;\")\n",
    "cur.execute(\"CREATE TABLE data_hstore (id SERIAL PRIMARY KEY, attributes hstore);\")\n",
    "conn.commit()\n",
    "\n",
    "def load_hstore_from_csv(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append((row,))\n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def test_latency(row_counts, csv_path):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = load_hstore_from_csv(csv_path, max_rows=n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO data_hstore (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        latency = (time.time() - start) * 1000\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "        results.append((n, latency))\n",
    "    return results\n",
    "\n",
    "#csv_path = \"D:/semesters/SEM7/COL868/benchmark/archive/Food_Supply_kcal_Data.csv\"\n",
    "csv_path = \"datasets\\yelp_business.csv\"\n",
    "row_counts = [10000, 50000, 100000,150346]\n",
    "\n",
    "results = test_latency(row_counts, csv_path)\n",
    "print(\"\\nLatency Results (HSTORE):\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d93fc",
   "metadata": {},
   "source": [
    "### Insertion into JSONB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b7946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost:5432/mydb\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS data_jsonb CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 rows in 733.94 ms\n",
      "Inserted 50000 rows in 4147.46 ms\n",
      "Inserted 100000 rows in 9341.43 ms\n",
      "Inserted 150346 rows in 14529.61 ms\n",
      "\n",
      "Latency Results (JSONB):\n",
      " 10000 rows →   733.94 ms\n",
      " 50000 rows →  4147.46 ms\n",
      "100000 rows →  9341.43 ms\n",
      "150346 rows → 14529.61 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\",\n",
    "    user=\"postgres\",\n",
    "    password=\"navya\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"DROP TABLE IF EXISTS data_jsonb;\")\n",
    "cur.execute(\"CREATE TABLE data_jsonb (id SERIAL PRIMARY KEY, attributes JSONB);\")\n",
    "conn.commit()\n",
    "\n",
    "def load_jsonb_from_csv(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append((json.dumps(row),))\n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def test_latency(row_counts, csv_path):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = load_jsonb_from_csv(csv_path, max_rows=n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO data_jsonb (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        latency = (time.time() - start) * 1000\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "        results.append((n, latency))\n",
    "    return results\n",
    "\n",
    "#csv_path = \"D:/semesters/SEM7/COL868/benchmark/archive/Food_Supply_kcal_Data.csv\"\n",
    "csv_path = \"datasets\\yelp_business.csv\"\n",
    "row_counts = [10000, 50000, 100000,150346]\n",
    "\n",
    "results = test_latency(row_counts, csv_path)\n",
    "print(\"\\nLatency Results (JSONB):\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf7b0f",
   "metadata": {},
   "source": [
    "### Insertion into postgresql-vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a89d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost:5432/mydb\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS data_vanilla CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d65c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 rows in 633.82 ms\n",
      "Inserted 50000 rows in 3902.62 ms\n",
      "Inserted 100000 rows in 7704.27 ms\n",
      "Inserted 150346 rows in 11972.24 ms\n",
      "\n",
      "Latency Results (Vanilla):\n",
      " 10000 rows →   633.82 ms\n",
      " 50000 rows →  3902.62 ms\n",
      "100000 rows →  7704.27 ms\n",
      "150346 rows → 11972.24 ms\n",
      "\n",
      "\n",
      "Sample Column Types Created:\n",
      "  business_id: text\n",
      "  name: text\n",
      "  address: text\n",
      "  city: text\n",
      "  state: text\n",
      "  postal_code: text\n",
      "  latitude: numeric\n",
      "  longitude: numeric\n",
      "  stars: numeric\n",
      "  review_count: integer\n",
      "  is_open: integer\n",
      "  categories: text\n",
      "  hours: text\n",
      "  attributes.ByAppointmentOnly: text\n",
      "  attributes.BusinessAcceptsCreditCards: text\n",
      "  hours.Monday: text\n",
      "  hours.Tuesday: text\n",
      "  hours.Wednesday: text\n",
      "  hours.Thursday: text\n",
      "  hours.Friday: text\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\",\n",
    "    user=\"postgres\",\n",
    "    password=\"navya\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"DROP TABLE IF EXISTS data_vanilla;\")\n",
    "cur.execute(\"CREATE TABLE data_vanilla (id SERIAL PRIMARY KEY);\")\n",
    "conn.commit()\n",
    "\n",
    "# Define proper column types for your dataset\n",
    "COLUMN_TYPES = {\n",
    "    'business_id': 'TEXT',\n",
    "    'name': 'TEXT',\n",
    "    'address': 'TEXT',\n",
    "    'city': 'TEXT',\n",
    "    'state': 'TEXT',\n",
    "    'postal_code': 'TEXT',\n",
    "    'latitude': 'NUMERIC',\n",
    "    'longitude': 'NUMERIC',\n",
    "    'stars': 'NUMERIC',\n",
    "    'review_count': 'INTEGER',\n",
    "    'is_open': 'INTEGER',\n",
    "    'categories': 'TEXT',\n",
    "    'hours': 'TEXT',\n",
    "    # Attributes - keep as TEXT to avoid conversion issues\n",
    "    'attributes.ByAppointmentOnly': 'TEXT',\n",
    "    'attributes.BusinessAcceptsCreditCards': 'TEXT',\n",
    "    'RestaurantsTakeOut': 'TEXT',\n",
    "    'RestaurantsPriceRange2': 'TEXT',\n",
    "}\n",
    "\n",
    "def infer_type(value):\n",
    "    \"\"\"Infer PostgreSQL type from sample value\"\"\"\n",
    "    if value is None or value == '' or value.lower() == 'none':\n",
    "        return 'TEXT'\n",
    "    \n",
    "    # Try integer\n",
    "    try:\n",
    "        int(value)\n",
    "        return 'INTEGER'\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # Try numeric\n",
    "    try:\n",
    "        float(value)\n",
    "        return 'NUMERIC'\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    return 'TEXT'\n",
    "\n",
    "def create_columns_from_csv(csv_file_path, sample_size=1000):\n",
    "    \"\"\"Create columns with inferred types\"\"\"\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = reader.fieldnames\n",
    "        \n",
    "        # Sample rows to infer types\n",
    "        samples = {col: [] for col in cols}\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            for col in cols:\n",
    "                val = row[col]\n",
    "                if val and val.lower() != 'none':  # Only valid values\n",
    "                    samples[col].append(val)\n",
    "        \n",
    "        # Create columns with inferred types\n",
    "        for col in cols:\n",
    "            # Use predefined type if available\n",
    "            if col in COLUMN_TYPES:\n",
    "                col_type = COLUMN_TYPES[col]\n",
    "            # Otherwise infer from samples\n",
    "            elif samples[col]:\n",
    "                col_type = infer_type(samples[col][0])\n",
    "            else:\n",
    "                col_type = 'TEXT'\n",
    "            \n",
    "            cur.execute(f'ALTER TABLE data_vanilla ADD COLUMN \"{col}\" {col_type};')\n",
    "        \n",
    "        conn.commit()\n",
    "        return cols\n",
    "\n",
    "def convert_value(value, col_name):\n",
    "    \"\"\"Convert value to appropriate type, return None for NULL values\"\"\"\n",
    "    # Handle NULL cases\n",
    "    if value is None or value == '' or value.lower() in ('none', 'null'):\n",
    "        return None\n",
    "    \n",
    "    col_type = COLUMN_TYPES.get(col_name, 'TEXT')\n",
    "    \n",
    "    try:\n",
    "        if col_type == 'INTEGER':\n",
    "            return int(float(value))  # Handle \"1.0\" -> 1\n",
    "        elif col_type == 'NUMERIC':\n",
    "            return float(value)\n",
    "        else:\n",
    "            return value\n",
    "    except (ValueError, TypeError):\n",
    "        return None  # Return NULL for conversion failures\n",
    "\n",
    "def load_rows(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = reader.fieldnames\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            # Convert values to proper types\n",
    "            converted_row = tuple(convert_value(row[col], col) for col in cols)\n",
    "            data.append(converted_row)\n",
    "            \n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def test_latency(row_counts, csv_path):\n",
    "    cols = create_columns_from_csv(csv_path)\n",
    "    results = []\n",
    "    \n",
    "    for n in row_counts:\n",
    "        # Clear table before each test\n",
    "        cur.execute(\"TRUNCATE TABLE data_vanilla RESTART IDENTITY;\")\n",
    "        conn.commit()\n",
    "        \n",
    "        rows = load_rows(csv_path, n)\n",
    "        start = time.time()\n",
    "        cols_quoted = ','.join(f'\"{c}\"' for c in cols)\n",
    "        \n",
    "        # Use execute_values with proper NULL handling\n",
    "        execute_values(\n",
    "            cur,\n",
    "            f'INSERT INTO data_vanilla ({cols_quoted}) VALUES %s',\n",
    "            rows,\n",
    "            template=None,\n",
    "            page_size=1000  # Batch inserts for better performance\n",
    "        )\n",
    "        conn.commit()\n",
    "        latency = (time.time() - start) * 1000\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "        results.append((n, latency))\n",
    "    \n",
    "    return results\n",
    "\n",
    "csv_path = r\"yelp_business.csv\"\n",
    "row_counts = [10000, 50000, 100000, 150346]\n",
    "\n",
    "try:\n",
    "    results = test_latency(row_counts, csv_path)\n",
    "    print(\"\\nLatency Results (Vanilla):\")\n",
    "    for n, t in results:\n",
    "        print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "    # Verify column types\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT column_name, data_type \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_name = 'data_vanilla' \n",
    "        AND column_name NOT IN ('id')\n",
    "        ORDER BY ordinal_position\n",
    "        LIMIT 20;\n",
    "    \"\"\")\n",
    "    print(\"\\n\\nSample Column Types Created:\")\n",
    "    for col, dtype in cur.fetchall():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    conn.rollback()\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55312ac",
   "metadata": {},
   "source": [
    "### Aggregate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7a689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " POSTGRESQL HSTORE vs JSONB vs VANILLA - AGGREGATION BENCHMARKS\n",
      "================================================================================\n",
      " Timestamp: 2025-11-07 13:23:46\n",
      " Dataset: Yelp Business\n",
      " Iterations per query: 5\n",
      "================================================================================\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TC25: COUNT WITH FILTERS\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "--- TEST 1: Simple boolean filter (is_open = 1) ---\n",
      "  Result: (119698,)\n",
      "VANILLA              → Avg:    69.13 ms | Min:    47.99 ms | Max:   133.54 ms\n",
      "  Result: (247203,)\n",
      "HSTORE               → Avg:   544.79 ms | Min:   454.38 ms | Max:   756.95 ms\n",
      "  Result: (247203,)\n",
      "JSONB                → Avg:   494.76 ms | Min:   406.34 ms | Max:   757.03 ms\n",
      "\n",
      "--- TEST 2: Sparse attribute filter (RestaurantsTakeOut) ---\n",
      "  Result: (52943,)\n",
      "VANILLA              → Avg:    66.62 ms | Min:    57.79 ms | Max:    86.33 ms\n",
      "  Result: (109310,)\n",
      "HSTORE               → Avg:   503.32 ms | Min:   433.33 ms | Max:   670.11 ms\n",
      "  Result: (109310,)\n",
      "JSONB                → Avg:   477.83 ms | Min:   426.08 ms | Max:   606.74 ms\n",
      "\n",
      "--- TEST 3: Multiple conditions ---\n",
      "  Result: (4065,)\n",
      "VANILLA              → Avg:    62.09 ms | Min:    49.58 ms | Max:    77.47 ms\n",
      "  Result: (8365,)\n",
      "HSTORE               → Avg:   584.41 ms | Min:   536.79 ms | Max:   651.13 ms\n",
      "  Result: (8365,)\n",
      "JSONB                → Avg:   565.42 ms | Min:   522.88 ms | Max:   609.28 ms\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TC26: GROUP BY EXTRACTED VALUES\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "--- TEST 1: High cardinality (state) ---\n",
      "VANILLA              → Avg:    70.90 ms | Min:    53.05 ms | Max:    84.37 ms\n",
      "HSTORE               → Avg:  1242.29 ms | Min:  1171.52 ms | Max:  1395.43 ms\n",
      "JSONB                → Avg:  1336.66 ms | Min:  1166.61 ms | Max:  1832.93 ms\n",
      "\n",
      "--- TEST 2: Medium cardinality (price range) ---\n",
      "VANILLA              → Avg:    66.62 ms | Min:    64.17 ms | Max:    68.92 ms\n",
      "HSTORE               → Avg:  1260.19 ms | Min:  1134.82 ms | Max:  1649.98 ms\n",
      "JSONB                → Avg:  1433.21 ms | Min:  1183.50 ms | Max:  2250.42 ms\n",
      "\n",
      "--- TEST 3: Low cardinality (is_open) ---\n",
      "  Results (2 rows):\n",
      "    (0, 30648)\n",
      "    (1, 119698)\n",
      "VANILLA              → Avg:    63.25 ms | Min:    48.77 ms | Max:    79.57 ms\n",
      "  Results (2 rows):\n",
      "    ('1', 247203)\n",
      "    ('0', 63143)\n",
      "HSTORE               → Avg:   896.26 ms | Min:   833.27 ms | Max:   983.40 ms\n",
      "  Results (2 rows):\n",
      "    ('1', 247203)\n",
      "    ('0', 63143)\n",
      "JSONB                → Avg:   850.21 ms | Min:   798.74 ms | Max:   878.09 ms\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TC27: AVG/SUM NUMERIC AGGREGATIONS\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "--- TEST 1: Simple aggregations ---\n",
      "  Result: (Decimal('3.6189075840866180'), Decimal('46.6839629734832662'), 5587977)\n",
      "VANILLA              → Avg:    66.64 ms | Min:    50.43 ms | Max:    83.08 ms\n",
      "  Result: (Decimal('3.6204698163048183'), Decimal('46.8655922460487939'), Decimal('11585315'))\n",
      "HSTORE               → Avg:   710.35 ms | Min:   668.57 ms | Max:   769.98 ms\n",
      "  Result: (Decimal('3.6204698163048183'), Decimal('46.8655922460487939'), Decimal('11585315'))\n",
      "JSONB                → Avg:   719.37 ms | Min:   679.93 ms | Max:   766.40 ms\n",
      "\n",
      "--- TEST 2: Grouped aggregation (avg by state) ---\n",
      "VANILLA              → Avg:    76.56 ms | Min:    60.44 ms | Max:    83.26 ms\n",
      "HSTORE               → Avg:  2636.67 ms | Min:  2380.38 ms | Max:  3320.05 ms\n",
      "JSONB                → Avg:  2727.69 ms | Min:  2516.30 ms | Max:  3217.57 ms\n",
      "\n",
      "--- TEST 3: Multiple aggregations ---\n",
      "  Result: (119698, Decimal('3.6189075840866180'), Decimal('1.0'), Decimal('5.0'), 5587977, Decimal('46.6839629734832662'))\n",
      "VANILLA              → Avg:    73.00 ms | Min:    48.49 ms | Max:    95.93 ms\n",
      "  Result: (247203, Decimal('3.6204698163048183'), Decimal('1.0'), Decimal('5.0'), Decimal('11585315'), Decimal('46.8655922460487939'))\n",
      "HSTORE               → Avg:  1010.68 ms | Min:   917.15 ms | Max:  1180.40 ms\n",
      "  Result: (247203, Decimal('3.6204698163048183'), Decimal('1.0'), Decimal('5.0'), Decimal('11585315'), Decimal('46.8655922460487939'))\n",
      "JSONB                → Avg:  1045.43 ms | Min:   979.99 ms | Max:  1082.52 ms\n",
      "\n",
      "================================================================================\n",
      " PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Average across all tests:\n",
      "  VANILLA:    68.31 ms  (1.00x) ← BASELINE\n",
      "  HSTORE:   1043.22 ms  (15.27x)\n",
      "  JSONB:    1072.28 ms  (15.70x)\n",
      "\n",
      "Test Success Rate:\n",
      "  VANILLA: 9/9 tests passed\n",
      "  HSTORE:  9/9 tests passed\n",
      "  JSONB:   9/9 tests passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "import sys\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\",\n",
    "    user=\"postgres\",\n",
    "    password=\"navya\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = False\n",
    "\n",
    "def get_cursor():\n",
    "    \"\"\"Get a fresh cursor and rollback any failed transactions\"\"\"\n",
    "    try:\n",
    "        conn.rollback()\n",
    "    except:\n",
    "        pass\n",
    "    return conn.cursor()\n",
    "\n",
    "cur = get_cursor()\n",
    "\n",
    "def time_query(label, query, iterations=5, show_result=True):\n",
    "    \"\"\"Run query multiple times and return average time\"\"\"\n",
    "    times = []\n",
    "    result_data = None\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        cur = get_cursor()\n",
    "        start = time.time()\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            if show_result and i == 0:\n",
    "                result_data = cur.fetchall()\n",
    "            else:\n",
    "                cur.fetchall()\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            conn.rollback()\n",
    "            cur.close()\n",
    "            return None\n",
    "        end = time.time()\n",
    "        elapsed = (end - start) * 1000\n",
    "        times.append(elapsed)\n",
    "        cur.close()\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    min_time = min(times)\n",
    "    max_time = max(times)\n",
    "    \n",
    "    if show_result and result_data is not None:\n",
    "        if len(result_data) == 1:\n",
    "            print(f\"  Result: {result_data[0]}\")\n",
    "        elif len(result_data) <= 5:\n",
    "            print(f\"  Results ({len(result_data)} rows):\")\n",
    "            for row in result_data:\n",
    "                print(f\"    {row}\")\n",
    "        else:\n",
    "            print(f\"  Results: {len(result_data)} rows returned\")\n",
    "    \n",
    "    print(f\"{label:<20} → Avg: {avg_time:>8.2f} ms | Min: {min_time:>8.2f} ms | Max: {max_time:>8.2f} ms\")\n",
    "    return avg_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" POSTGRESQL HSTORE vs JSONB vs VANILLA - AGGREGATION BENCHMARKS\")\n",
    "print(\"=\"*80)\n",
    "print(f\" Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\" Dataset: Yelp Business\")\n",
    "print(f\" Iterations per query: 5\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store results for summary\n",
    "results = {\n",
    "    'vanilla': [],\n",
    "    'hstore': [],\n",
    "    'jsonb': []\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TC25: COUNT WITH FILTERS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TC25: COUNT WITH FILTERS\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "print(\"\\n--- TEST 1: Simple boolean filter (is_open = 1) ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_vanilla WHERE is_open = 1;\n",
    "\"\"\"))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_hstore \n",
    "WHERE attributes->'is_open' = '1';\n",
    "\"\"\"))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_jsonb \n",
    "WHERE attributes->>'is_open' = '1';\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n--- TEST 2: Sparse attribute filter (RestaurantsTakeOut) ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_vanilla WHERE \"attributes.RestaurantsTakeOut\" = 'True';\n",
    "\"\"\"))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_hstore \n",
    "WHERE attributes->'attributes.RestaurantsTakeOut' = 'True';\n",
    "\"\"\"))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_jsonb \n",
    "WHERE attributes->>'attributes.RestaurantsTakeOut' = 'True';\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n--- TEST 3: Multiple conditions ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_vanilla WHERE is_open = 1 AND state = 'CA';\n",
    "\"\"\"))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_hstore \n",
    "WHERE attributes->'is_open' = '1' AND attributes->'state' = 'CA';\n",
    "\"\"\"))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT COUNT(*) FROM data_jsonb \n",
    "WHERE attributes->>'is_open' = '1' AND attributes->>'state' = 'CA';\n",
    "\"\"\"))\n",
    "\n",
    "# ============================================================================\n",
    "# TC26: GROUP BY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TC26: GROUP BY EXTRACTED VALUES\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "print(\"\\n--- TEST 1: High cardinality (state) ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT state, COUNT(*) FROM data_vanilla \n",
    "WHERE state IS NOT NULL\n",
    "GROUP BY state ORDER BY COUNT(*) DESC LIMIT 10;\n",
    "\"\"\", show_result=False))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT attributes->'state', COUNT(*) FROM data_hstore \n",
    "WHERE attributes->'state' IS NOT NULL\n",
    "GROUP BY attributes->'state' ORDER BY COUNT(*) DESC LIMIT 10;\n",
    "\"\"\", show_result=False))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT attributes->>'state', COUNT(*) FROM data_jsonb \n",
    "WHERE attributes->>'state' IS NOT NULL\n",
    "GROUP BY attributes->>'state' ORDER BY COUNT(*) DESC LIMIT 10;\n",
    "\"\"\", show_result=False))\n",
    "\n",
    "print(\"\\n--- TEST 2: Medium cardinality (price range) ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT \"attributes.RestaurantsPriceRange2\", COUNT(*) FROM data_vanilla \n",
    "WHERE \"attributes.RestaurantsPriceRange2\" IS NOT NULL\n",
    "GROUP BY \"attributes.RestaurantsPriceRange2\" ORDER BY COUNT(*) DESC;\n",
    "\"\"\", show_result=False))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT attributes->'attributes.RestaurantsPriceRange2', COUNT(*) FROM data_hstore \n",
    "WHERE attributes->'attributes.RestaurantsPriceRange2' IS NOT NULL\n",
    "GROUP BY attributes->'attributes.RestaurantsPriceRange2' ORDER BY COUNT(*) DESC;\n",
    "\"\"\", show_result=False))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT attributes->>'attributes.RestaurantsPriceRange2', COUNT(*) \n",
    "FROM data_jsonb \n",
    "WHERE attributes->>'attributes.RestaurantsPriceRange2' IS NOT NULL\n",
    "GROUP BY attributes->>'attributes.RestaurantsPriceRange2' \n",
    "ORDER BY COUNT(*) DESC;\n",
    "\"\"\", show_result=False))\n",
    "\n",
    "print(\"\\n--- TEST 3: Low cardinality (is_open) ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT is_open, COUNT(*) FROM data_vanilla GROUP BY is_open;\n",
    "\"\"\"))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT attributes->'is_open', COUNT(*) FROM data_hstore \n",
    "GROUP BY attributes->'is_open';\n",
    "\"\"\"))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT attributes->>'is_open', COUNT(*) FROM data_jsonb \n",
    "GROUP BY attributes->>'is_open';\n",
    "\"\"\"))\n",
    "\n",
    "# ============================================================================\n",
    "# TC27: NUMERIC AGGREGATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TC27: AVG/SUM NUMERIC AGGREGATIONS\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "print(\"\\n--- TEST 1: Simple aggregations ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT AVG(stars), AVG(review_count), SUM(review_count) \n",
    "FROM data_vanilla WHERE is_open = 1;\n",
    "\"\"\"))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT AVG((attributes->'stars')::numeric), \n",
    "       AVG((attributes->'review_count')::numeric),\n",
    "       SUM((attributes->'review_count')::numeric)\n",
    "FROM data_hstore WHERE attributes->'is_open' = '1';\n",
    "\"\"\"))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT AVG((attributes->>'stars')::numeric),\n",
    "       AVG((attributes->>'review_count')::numeric),\n",
    "       SUM((attributes->>'review_count')::numeric)\n",
    "FROM data_jsonb WHERE attributes->>'is_open' = '1';\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n--- TEST 2: Grouped aggregation (avg by state) ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT state, AVG(stars), COUNT(*) FROM data_vanilla \n",
    "WHERE state IS NOT NULL\n",
    "GROUP BY state HAVING COUNT(*) > 10 \n",
    "ORDER BY AVG(stars) DESC LIMIT 10;\n",
    "\"\"\", show_result=False))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT attributes->'state', AVG((attributes->'stars')::numeric), COUNT(*) \n",
    "FROM data_hstore \n",
    "WHERE attributes->'state' IS NOT NULL AND attributes->'stars' IS NOT NULL\n",
    "GROUP BY attributes->'state' HAVING COUNT(*) > 10 \n",
    "ORDER BY AVG((attributes->'stars')::numeric) DESC LIMIT 10;\n",
    "\"\"\", show_result=False))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT attributes->>'state', AVG((attributes->>'stars')::numeric), COUNT(*) \n",
    "FROM data_jsonb \n",
    "WHERE attributes->>'state' IS NOT NULL AND attributes->>'stars' IS NOT NULL\n",
    "GROUP BY attributes->>'state' HAVING COUNT(*) > 10 \n",
    "ORDER BY AVG((attributes->>'stars')::numeric) DESC LIMIT 10;\n",
    "\"\"\", show_result=False))\n",
    "\n",
    "print(\"\\n--- TEST 3: Multiple aggregations ---\")\n",
    "results['vanilla'].append(time_query(\"VANILLA\", \"\"\"\n",
    "SELECT COUNT(*), AVG(stars), MIN(stars), MAX(stars), \n",
    "       SUM(review_count), AVG(review_count)\n",
    "FROM data_vanilla WHERE is_open = 1;\n",
    "\"\"\"))\n",
    "results['hstore'].append(time_query(\"HSTORE\", \"\"\"\n",
    "SELECT COUNT(*), \n",
    "       AVG((attributes->'stars')::numeric),\n",
    "       MIN((attributes->'stars')::numeric),\n",
    "       MAX((attributes->'stars')::numeric),\n",
    "       SUM((attributes->'review_count')::numeric),\n",
    "       AVG((attributes->'review_count')::numeric)\n",
    "FROM data_hstore WHERE attributes->'is_open' = '1';\n",
    "\"\"\"))\n",
    "results['jsonb'].append(time_query(\"JSONB\", \"\"\"\n",
    "SELECT COUNT(*),\n",
    "       AVG((attributes->>'stars')::numeric),\n",
    "       MIN((attributes->>'stars')::numeric),\n",
    "       MAX((attributes->>'stars')::numeric),\n",
    "       SUM((attributes->>'review_count')::numeric),\n",
    "       AVG((attributes->>'review_count')::numeric)\n",
    "FROM data_jsonb WHERE attributes->>'is_open' = '1';\n",
    "\"\"\"))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vanilla_valid = [t for t in results['vanilla'] if t is not None]\n",
    "hstore_valid = [t for t in results['hstore'] if t is not None]\n",
    "jsonb_valid = [t for t in results['jsonb'] if t is not None]\n",
    "\n",
    "if vanilla_valid and hstore_valid and jsonb_valid:\n",
    "    vanilla_avg = sum(vanilla_valid) / len(vanilla_valid)\n",
    "    hstore_avg = sum(hstore_valid) / len(hstore_valid)\n",
    "    jsonb_avg = sum(jsonb_valid) / len(jsonb_valid)\n",
    "\n",
    "    print(f\"\\nAverage across all tests:\")\n",
    "    print(f\"  VANILLA: {vanilla_avg:>8.2f} ms  (1.00x) ← BASELINE\")\n",
    "    print(f\"  HSTORE:  {hstore_avg:>8.2f} ms  ({hstore_avg/vanilla_avg:.2f}x)\")\n",
    "    print(f\"  JSONB:   {jsonb_avg:>8.2f} ms  ({jsonb_avg/vanilla_avg:.2f}x)\")\n",
    "    \n",
    "    print(f\"\\nTest Success Rate:\")\n",
    "    print(f\"  VANILLA: {len(vanilla_valid)}/{len(results['vanilla'])} tests passed\")\n",
    "    print(f\"  HSTORE:  {len(hstore_valid)}/{len(results['hstore'])} tests passed\")\n",
    "    print(f\"  JSONB:   {len(jsonb_valid)}/{len(results['jsonb'])} tests passed\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Some queries failed. Check errors above.\")\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
