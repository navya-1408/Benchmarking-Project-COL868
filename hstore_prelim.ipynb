{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipython-sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54544085",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf3e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1735f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:///my_database.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7dfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ipython-sql psycopg2 prettytable==3.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace77125",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:priya123@localhost:5432/my_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f71e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade prettytable==3.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791c6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509381db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prettytable==3.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba9c90f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 rows in 13.53 ms\n",
      "Inserted 1000 rows in 52.99 ms\n",
      "Inserted 5000 rows in 303.17 ms\n",
      "Inserted 10000 rows in 836.14 ms\n",
      "Inserted 50000 rows in 3527.49 ms\n",
      "\n",
      "Latency Results:\n",
      "   100 rows →    13.53 ms\n",
      "  1000 rows →    52.99 ms\n",
      "  5000 rows →   303.17 ms\n",
      " 10000 rows →   836.14 ms\n",
      " 50000 rows →  3527.49 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from psycopg2.extras import execute_values, register_hstore\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"my_database\",\n",
    "    user=\"postgres\",\n",
    "    password=\"priya123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Enable hstore and register adapter\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS hstore;\")\n",
    "register_hstore(conn)  # <-- This is required!\n",
    "cur.execute(\"DROP TABLE IF EXISTS latency_test;\")\n",
    "cur.execute(\"CREATE TABLE latency_test (id SERIAL PRIMARY KEY, attributes hstore);\")\n",
    "conn.commit()\n",
    "\n",
    "# Function to create random key-value data\n",
    "def random_hstore(batch_size):\n",
    "    data = []\n",
    "    for _ in range(batch_size):\n",
    "        kv_pairs = {f\"key{j}\": ''.join(random.choices(string.ascii_lowercase, k=5))\n",
    "                    for j in range(5)}\n",
    "        data.append((kv_pairs,))\n",
    "    return data\n",
    "\n",
    "# Function to test latency for different row counts\n",
    "def test_latency(row_counts):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = random_hstore(n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO latency_test (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        end = time.time()\n",
    "        latency = (end - start) * 1000  # in milliseconds\n",
    "        results.append((n, latency))\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "row_counts = [100, 1000, 5000, 10000, 50000]\n",
    "#row_counts = [1, 2, 3, 4, 5]\n",
    "results = test_latency(row_counts)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLatency Results:\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e94742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 rows in 8.15 ms\n",
      "Inserted 1000 rows in 15.47 ms\n",
      "Inserted 5000 rows in 122.28 ms\n",
      "Inserted 10000 rows in 315.27 ms\n",
      "Inserted 50000 rows in 1787.58 ms\n",
      "\n",
      "Latency Results (JSONB):\n",
      "   100 rows →     8.15 ms\n",
      "  1000 rows →    15.47 ms\n",
      "  5000 rows →   122.28 ms\n",
      " 10000 rows →   315.27 ms\n",
      " 50000 rows →  1787.58 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from psycopg2.extras import Json\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"my_database\",\n",
    "    user=\"postgres\",\n",
    "    password=\"priya123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Drop table if exists and create a JSONB table\n",
    "cur.execute(\"DROP TABLE IF EXISTS latency_test_jsonb;\")\n",
    "cur.execute(\"CREATE TABLE latency_test_jsonb (id SERIAL PRIMARY KEY, attributes jsonb);\")\n",
    "conn.commit()\n",
    "\n",
    "# Function to create random key-value data\n",
    "def random_jsonb(batch_size):\n",
    "    data = []\n",
    "    for _ in range(batch_size):\n",
    "        kv_pairs = {f\"key{j}\": ''.join(random.choices(string.ascii_lowercase, k=5))\n",
    "                    for j in range(5)}\n",
    "        data.append((Json(kv_pairs),))  # wrap dict in Json()\n",
    "    return data\n",
    "\n",
    "# Function to test latency for different row counts\n",
    "def test_latency_jsonb(row_counts):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = random_jsonb(n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO latency_test_jsonb (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        end = time.time()\n",
    "        latency = (end - start) * 1000  # in milliseconds\n",
    "        results.append((n, latency))\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "#row_counts = [1, 2, 3, 4, 5]\n",
    "row_counts = [100, 1000, 5000, 10000, 50000]\n",
    "results = test_latency_jsonb(row_counts)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLatency Results (JSONB):\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee991c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 rows in 133.87 ms\n",
      "Inserted 50000 rows in 17999.87 ms\n",
      "Inserted 102000 rows in 23353.14 ms\n",
      "\n",
      "Latency Results:\n",
      "  1000 rows →   133.87 ms\n",
      " 50000 rows → 17999.87 ms\n",
      "102000 rows → 23353.14 ms\n",
      "\n",
      "Executing average query for 'Animal fats'...\n",
      "Found 153000 unique countries\n",
      "Query execution time: 404.82 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "from psycopg2.extras import execute_values, register_hstore\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"my_database\",\n",
    "    user=\"postgres\",\n",
    "    password=\"priya123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Enable hstore and register adapter\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS hstore;\")\n",
    "register_hstore(conn)\n",
    "cur.execute(\"DROP TABLE IF EXISTS latency_test;\")\n",
    "cur.execute(\"CREATE TABLE latency_test (id SERIAL PRIMARY KEY, attributes hstore);\")\n",
    "conn.commit()\n",
    "\n",
    "# Function to read data from CSV file\n",
    "def load_hstore_from_csv(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            # Each row is a dict: {'key1': 'apple', 'key2': 'banana', ...}\n",
    "            data.append((row,))  # Must be tuple for execute_values\n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "# Function to test latency for different row counts\n",
    "def test_latency(row_counts, csv_path):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = load_hstore_from_csv(csv_path, max_rows=n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO latency_test (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        end = time.time()\n",
    "        latency = (end - start) * 1000  # ms\n",
    "        results.append((n, latency))\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "    return results\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Run experiments\n",
    "\n",
    "csv_path = \"D:/semesters/SEM7/COL868/benchmark/archive/Food_Supply_kcal_Data.csv\"\n",
    "  # <-- put your CSV path here\n",
    "row_counts = [1000, 50000, 102000]   # can adjust as per file size\n",
    "\n",
    "results = test_latency(row_counts, csv_path)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLatency Results:\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "'''try:\n",
    "    cur.execute(\"SELECT AVG((attributes -> 'Animal fats')::float) FROM latency_test;\")\n",
    "    avg_animal_fats = cur.fetchone()[0]\n",
    "    print(f\"\\nAverage value of 'Animal fats': {avg_animal_fats:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError querying 'Animal fats':\", e)'''\n",
    "print(\"\\nExecuting average query for 'Animal fats'...\")\n",
    "'''query = \"\"\"\n",
    "    SELECT AVG((attributes -> 'Animal fats')::float) FROM latency_test;\n",
    "\"\"\"'''\n",
    "query = \"SELECT attributes -> 'Country' AS country FROM latency_test;\"\n",
    "start_time = time.time()\n",
    "cur.execute(query)\n",
    "countries = cur.fetchall()\n",
    "#avg_animal_fats = cur.fetchone()[0]\n",
    "end_time = time.time()\n",
    "\n",
    "query_latency = (end_time - start_time) * 1000  # ms\n",
    "#print(f\"Average value of 'Animal fats': {avg_animal_fats:.2f}\")\n",
    "print(f\"Found {len(countries)} unique countries\")\n",
    "print(f\"Query execution time: {query_latency:.2f} ms\")\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f481984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 rows in 304.99 ms\n",
      "Inserted 50000 rows in 11591.22 ms\n",
      "Inserted 102000 rows in 23966.04 ms\n",
      "\n",
      "Latency Results:\n",
      "  1000 rows →   304.99 ms\n",
      " 50000 rows → 11591.22 ms\n",
      "102000 rows → 23966.04 ms\n",
      "\n",
      "Executing average query for 'Animal fats'...\n",
      "\n",
      "Executing update query for 'Animal fats' where Country='India'...\n",
      "Update executed successfully in 225.39 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "from psycopg2.extras import execute_values, register_hstore\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"my_database\",\n",
    "    user=\"postgres\",\n",
    "    password=\"priya123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Enable hstore and register adapter\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS hstore;\")\n",
    "register_hstore(conn)\n",
    "cur.execute(\"DROP TABLE IF EXISTS latency_test;\")\n",
    "cur.execute(\"CREATE TABLE latency_test (id SERIAL PRIMARY KEY, attributes hstore);\")\n",
    "conn.commit()\n",
    "\n",
    "# Function to read data from CSV file\n",
    "def load_hstore_from_csv(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            # Each row is a dict: {'key1': 'apple', 'key2': 'banana', ...}\n",
    "            data.append((row,))  # Must be tuple for execute_values\n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "# Function to test latency for different row counts\n",
    "def test_latency(row_counts, csv_path):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = load_hstore_from_csv(csv_path, max_rows=n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO latency_test (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        end = time.time()\n",
    "        latency = (end - start) * 1000  # ms\n",
    "        results.append((n, latency))\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "    return results\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Run experiments\n",
    "\n",
    "csv_path = \"D:/semesters/SEM7/COL868/benchmark/archive/Food_Supply_kcal_Data.csv\"\n",
    "  # <-- put your CSV path here\n",
    "row_counts = [1000, 50000, 102000]   # can adjust as per file size\n",
    "\n",
    "results = test_latency(row_counts, csv_path)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLatency Results:\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "\n",
    "'''try:\n",
    "    cur.execute(\"SELECT AVG((attributes -> 'Animal fats')::float) FROM latency_test;\")\n",
    "    avg_animal_fats = cur.fetchone()[0]\n",
    "    print(f\"\\nAverage value of 'Animal fats': {avg_animal_fats:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError querying 'Animal fats':\", e)'''\n",
    "print(\"\\nExecuting average query for 'Animal fats'...\")\n",
    "'''query = \"\"\"\n",
    "    SELECT AVG((attributes -> 'Animal fats')::float) FROM latency_test;\n",
    "\"\"\"'''\n",
    "print(\"\\nExecuting update query for 'Animal fats' where Country='India'...\")\n",
    "\n",
    "query = \"\"\"\n",
    "    UPDATE latency_test\n",
    "    SET attributes = attributes || hstore('Animal fats', '2000')\n",
    "    WHERE attributes -> 'Country' = 'India';\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "end_time = time.time()\n",
    "\n",
    "update_latency = (end_time - start_time) * 1000  # milliseconds\n",
    "print(f\"Update executed successfully in {update_latency:.2f} ms\")\n",
    "\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35b05d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 rows in 115.08 ms\n",
      "Inserted 50000 rows in 8392.55 ms\n",
      "Inserted 102000 rows in 13011.63 ms\n",
      "\n",
      "Latency Results (JSONB):\n",
      "  1000 rows →   115.08 ms\n",
      " 50000 rows →  8392.55 ms\n",
      "102000 rows → 13011.63 ms\n",
      "\n",
      "Executing average query for 'Animal fats'...\n",
      "Found 153000 unique countries\n",
      "Query execution time: 177.85 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "from psycopg2.extras import Json, execute_values\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"my_database\",\n",
    "    user=\"postgres\",\n",
    "    password=\"priya123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Drop table if exists and create JSONB table\n",
    "cur.execute(\"DROP TABLE IF EXISTS latency_test_jsonb;\")\n",
    "cur.execute(\"CREATE TABLE latency_test_jsonb (id SERIAL PRIMARY KEY, attributes JSONB);\")\n",
    "conn.commit()\n",
    "\n",
    "# Function to read data from CSV file\n",
    "def load_jsonb_from_csv(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            # Convert each row (dict) to JSONB\n",
    "            data.append((Json(row),))  # must wrap in Json() for PostgreSQL\n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "# Function to test latency for different row counts\n",
    "def test_latency_jsonb(row_counts, csv_path):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = load_jsonb_from_csv(csv_path, max_rows=n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO latency_test_jsonb (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        end = time.time()\n",
    "        latency = (end - start) * 1000  # milliseconds\n",
    "        results.append((n, latency))\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "csv_path = \"D:/semesters/SEM7/COL868/benchmark/archive/Food_Supply_kcal_Data.csv\"  # your CSV path\n",
    "row_counts = [1000, 50000, 102000]  # adjust as needed\n",
    "\n",
    "results = test_latency_jsonb(row_counts, csv_path)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLatency Results (JSONB):\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "'''try:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT AVG((attributes ->> 'Animal fats')::float)\n",
    "        FROM latency_test_jsonb;\n",
    "    \"\"\")\n",
    "    avg_animal_fats = cur.fetchone()[0]\n",
    "    print(f\"\\nAverage value of 'Animal fats': {avg_animal_fats:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError querying 'Animal fats':\", e)'''\n",
    "print(\"\\nExecuting average query for 'Animal fats'...\")\n",
    "'''query = \"\"\"\n",
    "    SELECT AVG((attributes ->> 'Animal fats')::float)\n",
    "    FROM latency_test_jsonb;\n",
    "\"\"\"'''\n",
    "query = \"\"\"\n",
    "    SELECT  attributes ->> 'Country' AS country\n",
    "    FROM latency_test_jsonb;\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "cur.execute(query)\n",
    "countries = cur.fetchall()\n",
    "#avg_animal_fats = cur.fetchone()[0]\n",
    "end_time = time.time()\n",
    "\n",
    "query_latency = (end_time - start_time) * 1000  # ms\n",
    "#print(f\"Average value of 'Animal fats': {avg_animal_fats:.2f}\")\n",
    "print(f\"Found {len(countries)} unique countries\")\n",
    "print(f\"Query execution time: {query_latency:.2f} ms\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b36d7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 rows in 120.90 ms\n",
      "Inserted 50000 rows in 6667.35 ms\n",
      "Inserted 102000 rows in 11090.75 ms\n",
      "\n",
      "Latency Results (JSONB):\n",
      "  1000 rows →   120.90 ms\n",
      " 50000 rows →  6667.35 ms\n",
      "102000 rows → 11090.75 ms\n",
      "\n",
      "Executing average query for 'Animal fats'...\n",
      "Update time: 359.40 ms\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import csv\n",
    "from psycopg2.extras import Json, execute_values\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"my_database\",\n",
    "    user=\"postgres\",\n",
    "    password=\"priya123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Drop table if exists and create JSONB table\n",
    "cur.execute(\"DROP TABLE IF EXISTS latency_test_jsonb;\")\n",
    "cur.execute(\"CREATE TABLE latency_test_jsonb (id SERIAL PRIMARY KEY, attributes JSONB);\")\n",
    "conn.commit()\n",
    "\n",
    "# Function to read data from CSV file\n",
    "def load_jsonb_from_csv(csv_file_path, max_rows=None):\n",
    "    data = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            # Convert each row (dict) to JSONB\n",
    "            data.append((Json(row),))  # must wrap in Json() for PostgreSQL\n",
    "            if max_rows and len(data) >= max_rows:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "# Function to test latency for different row counts\n",
    "def test_latency_jsonb(row_counts, csv_path):\n",
    "    results = []\n",
    "    for n in row_counts:\n",
    "        batch = load_jsonb_from_csv(csv_path, max_rows=n)\n",
    "        start = time.time()\n",
    "        execute_values(cur, \"INSERT INTO latency_test_jsonb (attributes) VALUES %s\", batch)\n",
    "        conn.commit()\n",
    "        end = time.time()\n",
    "        latency = (end - start) * 1000  # milliseconds\n",
    "        results.append((n, latency))\n",
    "        print(f\"Inserted {n} rows in {latency:.2f} ms\")\n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "csv_path = \"D:/semesters/SEM7/COL868/benchmark/archive/Food_Supply_kcal_Data.csv\"  # your CSV path\n",
    "row_counts = [1000, 50000, 102000]  # adjust as needed\n",
    "\n",
    "results = test_latency_jsonb(row_counts, csv_path)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLatency Results (JSONB):\")\n",
    "for n, t in results:\n",
    "    print(f\"{n:6d} rows → {t:8.2f} ms\")\n",
    "'''try:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT AVG((attributes ->> 'Animal fats')::float)\n",
    "        FROM latency_test_jsonb;\n",
    "    \"\"\")\n",
    "    avg_animal_fats = cur.fetchone()[0]\n",
    "    print(f\"\\nAverage value of 'Animal fats': {avg_animal_fats:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError querying 'Animal fats':\", e)'''\n",
    "print(\"\\nExecuting average query for 'Animal fats'...\")\n",
    "'''query = \"\"\"\n",
    "    SELECT AVG((attributes ->> 'Animal fats')::float)\n",
    "    FROM latency_test_jsonb;\n",
    "\"\"\"'''\n",
    "query = \"\"\"\n",
    "    UPDATE latency_test_jsonb\n",
    "    SET attributes = jsonb_set(attributes, '{Animal fats}', '2000'::jsonb)\n",
    "    WHERE attributes ->> 'Country' = 'India';\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Update time: {(end - start) * 1000:.2f} ms\")\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
